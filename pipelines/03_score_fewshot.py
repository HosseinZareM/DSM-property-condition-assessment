import os
import json
import pandas as pd
from src.data_loader import DataLoader
from src.providers import get_provider
from src.config import Config
from src.data_loader import DataLoader

def select_gold_standard_examples(df_annotations, examples_per_score=1):
    """
    Select representative images for each score (1-5) to use as examples.
    
    Args:
        df_annotations: DataFrame with annotations and expert scores
        examples_per_score: How many examples to select per score
        
    Returns:
        Dictionary mapping score -> list of image paths
    """
    gold_standards = {}
    
    for score in [1, 2, 3, 4, 5]:
        score_images = df_annotations[
            (df_annotations['expert_score'] == str(score)) & 
            (df_annotations['image_path'].apply(os.path.exists))
        ]
        
        if len(score_images) > 0:
            # Select first N examples (or random sample)
            selected = score_images.head(examples_per_score)
            gold_standards[score] = selected['image_path'].tolist()
        else:
            gold_standards[score] = []
    
    return gold_standards

def build_fewshot_prompt(base_prompt, gold_standards, provider):
    """
    Build a few-shot prompt by adding example images and their scores.
    
    Args:
        base_prompt: The zero-shot scoring prompt
        gold_standards: Dictionary mapping score -> list of image paths
        provider: VLM provider instance (for encoding images if needed)
        
    Returns:
        Enhanced prompt string (or structured format depending on provider)
    """
    # For now, we'll create a text-based few-shot prompt
    # Some providers may need image embeddings injected differently
    
    fewshot_text = base_prompt + "\n\n## Examples:\n\n"
    
    for score in [1, 2, 3, 4, 5]:
        if score in gold_standards and gold_standards[score]:
            example_path = gold_standards[score][0]
            fewshot_text += f"Example Score {score}: See image '{os.path.basename(example_path)}'\n"
    
    fewshot_text += "\nNow analyze the target image using the same criteria as the examples above."
    
    return fewshot_text

def score_with_fewshot(image_paths, provider_name="openai", examples_per_score=1):
    """
    Score images using few-shot learning with gold standard examples.
    
    Args:
        image_paths: List of target image paths to score
        provider_name: VLM provider to use
        examples_per_score: Number of examples per score category
        
    Returns:
        DataFrame with scoring results
    """
    # Load annotations to get gold standards
    loader = DataLoader()
    df_annotations = loader.load_annotations()
    df_scored = df_annotations[df_annotations['expert_score'].notna()]
    
    # Select gold standard examples
    gold_standards = select_gold_standard_examples(df_scored, examples_per_score)
    
    # Load base prompt
    prompt_path = os.path.join(Config.PROMPTS_DIR, "prompt_zero_shot.txt")
    with open(prompt_path, "r") as f:
        base_prompt = f.read()
    
    provider = get_provider(provider_name)
    
    # Build few-shot prompt
    fewshot_prompt = build_fewshot_prompt(base_prompt, gold_standards, provider)
    
    results = []
    
    for img_path in image_paths:
        if not os.path.exists(img_path):
            results.append({
                "image_path": img_path,
                "provider": provider_name,
                "error": "File not found"
            })
            continue
        
        try:
            # For providers that support multiple images, we could inject examples here
            # For now, we'll use text-based few-shot prompting
            response = provider.analyze(img_path, fewshot_prompt)
            
            if response:
                # Parse JSON response
                clean_response = response.strip()
                if "```json" in clean_response:
                    clean_response = clean_response.split("```json")[1].split("```")[0].strip()
                elif "```" in clean_response:
                    clean_response = clean_response.split("```")[1].split("```")[0].strip()
                
                try:
                    parsed = json.loads(clean_response)
                    parsed["image_path"] = img_path
                    parsed["provider"] = provider_name
                    parsed["method"] = "fewshot"
                    results.append(parsed)
                except json.JSONDecodeError:
                    results.append({
                        "image_path": img_path,
                        "provider": provider_name,
                        "error": "JSON parse error",
                        "raw_response": response
                    })
            else:
                results.append({
                    "image_path": img_path,
                    "provider": provider_name,
                    "error": "Empty response"
                })
        except Exception as e:
            results.append({
                "image_path": img_path,
                "provider": provider_name,
                "error": str(e)
            })
    
    return pd.DataFrame(results)

if __name__ == "__main__":
    # Load annotations
    loader = DataLoader()
    df = loader.load_annotations()
    
    # Get scored images for testing
    scored_images = [img for img in df[df['expert_score'].notna()]['image_path'].tolist() 
                     if os.path.exists(img)]
    
    print(f"Found {len(scored_images)} scored images")
    
    # Test few-shot scoring
    print("\n=== Running Few-Shot Scoring ===")
    results = score_with_fewshot(scored_images[:10], provider_name="openai")  # Test on 10 first
    
    # Save results
    output_path = os.path.join(Config.OUTPUTS_DIR, "fewshot_scores.csv")
    os.makedirs(Config.OUTPUTS_DIR, exist_ok=True)
    results.to_csv(output_path, index=False)
    print(f"\nâœ… Results saved to: {output_path}")

