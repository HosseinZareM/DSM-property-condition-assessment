{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Property Scoring - Visual Analysis\n",
    "\n",
    "This notebook performs zero-shot property condition scoring using Vision Language Models (VLMs) without any examples.\n",
    "\n",
    "We'll:\n",
    "1. Load annotations and scoring prompt\n",
    "2. Run scoring with different VLM providers\n",
    "3. Compare predictions vs ground truth\n",
    "4. Visualize results with confusion matrices and distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, Markdown\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.display import display, Markdown\n",
    "from src.data_loader import DataLoader\n",
    "from src.providers import get_provider\n",
    "from src.config import Config\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "loader = DataLoader()\n",
    "df = loader.load_annotations()\n",
    "\n",
    "# Filter to only images with expert scores\n",
    "scored_df = df[df['expert_score'].notna()].copy()\n",
    "scored_df['expert_score'] = scored_df['expert_score'].astype(int)\n",
    "\n",
    "# Filter to existing images\n",
    "scored_df = scored_df[scored_df['image_path'].apply(os.path.exists)]\n",
    "\n",
    "print(f\"\ud83d\udcca Total images with expert scores: {len(scored_df)}\")\n",
    "print(f\"\\n\ud83d\udcca Score distribution:\")\n",
    "print(scored_df['expert_score'].value_counts().sort_index())\n",
    "\n",
    "# Sample for testing (use all for full run)\n",
    "SAMPLE_SIZE = 4  # Change to None to use all images\n",
    "if SAMPLE_SIZE:\n",
    "    scored_df = scored_df.sample(n=min(SAMPLE_SIZE, len(scored_df)), random_state=42)\n",
    "    print(f\"\\n\ud83c\udfb2 Using sample of {len(scored_df)} images for testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scoring prompt\n",
    "prompt_path = os.path.join(Config.PROMPTS_DIR, \"prompt_zero_shot.txt\")\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    scoring_prompt = f.read()\n",
    "\n",
    "print(\"Scoring Prompt (first 500 chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(scoring_prompt[:500] + \"...\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_from_response(response):\n",
    "    \"\"\"Extract score from VLM response\"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try to find JSON in response\n",
    "        if \"```json\" in response:\n",
    "            json_str = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            parsed = json.loads(json_str)\n",
    "            return parsed.get(\"score\") or parsed.get(\"overall_score\")\n",
    "        elif \"{\" in response and \"score\" in response.lower():\n",
    "            # Try to extract JSON object\n",
    "            json_match = re.search(r'\\{[^{}]*\"score\"[^{}]*\\}', response)\n",
    "            if json_match:\n",
    "                parsed = json.loads(json_match.group())\n",
    "                return parsed.get(\"score\")\n",
    "        \n",
    "        # Try to find score number\n",
    "        score_match = re.search(r'\"score\"\\s*:\\s*(\\d)', response, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            return int(score_match.group(1))\n",
    "        \n",
    "        # Try to find standalone score\n",
    "        score_match = re.search(r'\\bscore[:\\s]+(\\d)\\b', response, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            return int(score_match.group(1))\n",
    "        \n",
    "        # Try to find number 1-5\n",
    "        score_match = re.search(r'\\b([1-5])\\b', response)\n",
    "        if score_match:\n",
    "            return int(score_match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different providers\n",
    "providers_to_test = [\"local\", \"openai\", \"google\", \"together\"]  # Add/remove as needed\n",
    "results_by_provider = {}\n",
    "\n",
    "for provider_name in providers_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {provider_name.upper()} provider\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        provider = get_provider(provider_name)\n",
    "        print(f\"\u2705 Model: {provider.model_name}\")\n",
    "        \n",
    "        results = []\n",
    "        for idx, (_, row) in enumerate(scored_df.iterrows(), 1):\n",
    "            img_path = row['image_path']\n",
    "            expert_score = row['expert_score']\n",
    "            \n",
    "            if idx % 10 == 0:\n",
    "                print(f\"  Progress: {idx}/{len(scored_df)}\")\n",
    "            \n",
    "            try:\n",
    "                response = provider.analyze(img_path, scoring_prompt)\n",
    "                predicted_score = extract_score_from_response(response)\n",
    "                \n",
    "                results.append({\n",
    "                    \"image_path\": img_path,\n",
    "                    \"file_name\": os.path.basename(img_path),\n",
    "                    \"expert_score\": expert_score,\n",
    "                    \"predicted_score\": predicted_score,\n",
    "                    \"provider\": provider_name,\n",
    "                    \"model\": provider.model_name,\n",
    "                    \"raw_response\": response[:200] if response else None\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"image_path\": img_path,\n",
    "                    \"file_name\": os.path.basename(img_path),\n",
    "                    \"expert_score\": expert_score,\n",
    "                    \"predicted_score\": None,\n",
    "                    \"provider\": provider_name,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        results_by_provider[provider_name] = pd.DataFrame(results)\n",
    "        print(f\"\u2705 Completed {provider_name}: {len(results)} images processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error with {provider_name}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for each provider\n",
    "for provider_name, results_df in results_by_provider.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{provider_name.upper()} Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(results_df.head(10))\n",
    "    \n",
    "    # Statistics\n",
    "    valid_predictions = results_df[results_df['predicted_score'].notna()]\n",
    "    if len(valid_predictions) > 0:\n",
    "        print(f\"\\n\u2705 Valid predictions: {len(valid_predictions)}/{len(results_df)}\")\n",
    "        print(f\"\ud83d\udcca Predicted score distribution:\")\n",
    "        print(valid_predictions['predicted_score'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs ground truth\n",
    "for provider_name, results_df in results_by_provider.items():\n",
    "    valid_df = results_df[results_df['predicted_score'].notna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Score distribution comparison\n",
    "    expert_counts = valid_df['expert_score'].value_counts().sort_index()\n",
    "    pred_counts = valid_df['predicted_score'].value_counts().sort_index()\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Expert Score': expert_counts,\n",
    "        'Predicted Score': pred_counts\n",
    "    }).fillna(0)\n",
    "    \n",
    "    comparison.plot(kind='bar', ax=axes[0], color=['skyblue', 'coral'])\n",
    "    axes[0].set_title(f'{provider_name.upper()}: Score Distribution')\n",
    "    axes[0].set_xlabel('Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(valid_df['expert_score'], valid_df['predicted_score'], labels=[1,2,3,4,5])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1], \n",
    "                xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "    axes[1].set_title(f'{provider_name.upper()}: Confusion Matrix')\n",
    "    axes[1].set_xlabel('Predicted Score')\n",
    "    axes[1].set_ylabel('Expert Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n{classification_report(valid_df['expert_score'], valid_df['predicted_score'], labels=[1,2,3,4,5])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all providers side by side\n",
    "if len(results_by_provider) > 1:\n",
    "    fig, axes = plt.subplots(1, len(results_by_provider), figsize=(5*len(results_by_provider), 5))\n",
    "    if len(results_by_provider) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (provider_name, results_df) in enumerate(results_by_provider.items()):\n",
    "        valid_df = results_df[results_df['predicted_score'].notna()]\n",
    "        \n",
    "        if len(valid_df) > 0:\n",
    "            # Scatter plot: predicted vs expert\n",
    "            axes[idx].scatter(valid_df['expert_score'], valid_df['predicted_score'], alpha=0.5)\n",
    "            axes[idx].plot([1, 5], [1, 5], 'r--', label='Perfect prediction')\n",
    "            axes[idx].set_xlabel('Expert Score')\n",
    "            axes[idx].set_ylabel('Predicted Score')\n",
    "            axes[idx].set_title(f'{provider_name.upper()}\\nAccuracy: {np.mean(valid_df[\"expert_score\"] == valid_df[\"predicted_score\"]):.2%}')\n",
    "            axes[idx].legend()\n",
    "            axes[idx].set_xlim(0.5, 5.5)\n",
    "            axes[idx].set_ylim(0.5, 5.5)\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Config.OUTPUTS_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for provider_name, results_df in results_by_provider.items():\n",
    "    output_path = os.path.join(output_dir, f\"zeroshot_scores_{provider_name}.csv\")\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\u2705 {provider_name} results saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}