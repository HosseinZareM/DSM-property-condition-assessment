{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Property Scoring - Visual Analysis\n",
    "\n",
    "This notebook performs few-shot property condition scoring using Vision Language Models (VLMs) with example images and their annotations.\n",
    "\n",
    "We'll:\n",
    "1. Select gold standard examples for each score (1-5)\n",
    "2. Build few-shot prompts with examples\n",
    "3. Run scoring on test images\n",
    "4. Compare with zero-shot results (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import base64\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.display import display, Image, Markdown\n",
    "from src.data_loader import DataLoader\n",
    "from src.providers import get_provider\n",
    "from src.config import Config\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "loader = DataLoader()\n",
    "df = loader.load_annotations()\n",
    "\n",
    "# Filter to only images with expert scores\n",
    "scored_df = df[df['expert_score'].notna()].copy()\n",
    "scored_df['expert_score'] = scored_df['expert_score'].astype(int)\n",
    "scored_df = scored_df[scored_df['image_path'].apply(os.path.exists)]\n",
    "\n",
    "print(f\"\ud83d\udcca Total images with expert scores: {len(scored_df)}\")\n",
    "print(f\"\\n\ud83d\udcca Score distribution:\")\n",
    "print(scored_df['expert_score'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_gold_standard_examples(df_annotations, examples_per_score=1):\n",
    "    \"\"\"Select representative examples for each score\"\"\"\n",
    "    gold_standards = []\n",
    "    \n",
    "    for score in [1, 2, 3, 4, 5]:\n",
    "        score_images = df_annotations[df_annotations['expert_score'] == score]\n",
    "        \n",
    "        if len(score_images) > 0:\n",
    "            # Select random examples for this score\n",
    "            selected = score_images.sample(n=min(examples_per_score, len(score_images)), random_state=42)\n",
    "            \n",
    "            for _, row in selected.iterrows():\n",
    "                gold_standards.append({\n",
    "                    'score': score,\n",
    "                    'image_path': row['image_path'],\n",
    "                    'file_name': row['file_name']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(gold_standards)\n",
    "\n",
    "# Select gold standard examples\n",
    "EXAMPLES_PER_SCORE = 2  # Number of examples per score\n",
    "gold_standards = select_gold_standard_examples(scored_df, examples_per_score=EXAMPLES_PER_SCORE)\n",
    "\n",
    "print(f\"\u2705 Selected {len(gold_standards)} gold standard examples:\")\n",
    "print(gold_standards.groupby('score').size())\n",
    "print(\"\\nGold standard examples:\")\n",
    "display(gold_standards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base scoring prompt\n",
    "prompt_path = os.path.join(Config.PROMPTS_DIR, \"prompt_zero_shot.txt\")\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    base_prompt = f.read()\n",
    "\n",
    "print(\"Base Prompt (first 300 chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(base_prompt[:300] + \"...\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fewshot_prompt(base_prompt, gold_standards_df, provider_name=\"openai\"):\n",
    "    \"\"\"Build few-shot prompt with examples\"\"\"\n",
    "    # For now, create a text-based few-shot prompt\n",
    "    # Note: Some providers (like OpenAI) support image examples in messages\n",
    "    # This is a simplified version\n",
    "    \n",
    "    examples_text = \"\\n\\n## Examples:\\n\\n\"\n",
    "    \n",
    "    for _, example in gold_standards_df.iterrows():\n",
    "        examples_text += f\"Example - Score {example['score']}:\\n\"\n",
    "        examples_text += f\"Image: {example['file_name']}\\n\"\n",
    "        examples_text += f\"This property received a score of {example['score']}.\\n\\n\"\n",
    "    \n",
    "    fewshot_prompt = base_prompt + examples_text\n",
    "    fewshot_prompt += \"\\n\\nNow analyze the provided image and assign a score following the same criteria.\"\n",
    "    \n",
    "    return fewshot_prompt\n",
    "\n",
    "fewshot_prompt = build_fewshot_prompt(base_prompt, gold_standards)\n",
    "print(\"Few-shot prompt created (length:\", len(fewshot_prompt), \"chars)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_from_response(response):\n",
    "    \"\"\"Extract score from VLM response\"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if \"```json\" in response:\n",
    "            json_str = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            parsed = json.loads(json_str)\n",
    "            return parsed.get(\"score\") or parsed.get(\"overall_score\")\n",
    "        elif \"{\" in response and \"score\" in response.lower():\n",
    "            json_match = re.search(r'\\{[^{}]*\"score\"[^{}]*\\}', response)\n",
    "            if json_match:\n",
    "                parsed = json.loads(json_match.group())\n",
    "                return parsed.get(\"score\")\n",
    "        \n",
    "        score_match = re.search(r'\"score\"\\s*:\\s*(\\d)', response, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            return int(score_match.group(1))\n",
    "        \n",
    "        score_match = re.search(r'\\bscore[:\\s]+(\\d)\\b', response, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            return int(score_match.group(1))\n",
    "        \n",
    "        score_match = re.search(r'\\b([1-5])\\b', response)\n",
    "        if score_match:\n",
    "            return int(score_match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run few-shot scoring on test images\n",
    "provider_name = \"openai\"  # Change as needed\n",
    "provider = get_provider(provider_name)\n",
    "\n",
    "# Use images not in gold standards for testing\n",
    "test_images = scored_df[~scored_df['file_name'].isin(gold_standards['file_name'])].copy()\n",
    "test_images = test_images.sample(n=min(30, len(test_images)), random_state=42)\n",
    "\n",
    "print(f\"\ud83e\uddea Testing on {len(test_images)} images (excluding gold standards)\")\n",
    "print(f\"\u2705 Using provider: {provider_name} ({provider.model_name})\")\n",
    "\n",
    "results = []\n",
    "for idx, (_, row) in enumerate(test_images.iterrows(), 1):\n",
    "    img_path = row['image_path']\n",
    "    expert_score = row['expert_score']\n",
    "    \n",
    "    if idx % 5 == 0:\n",
    "        print(f\"  Progress: {idx}/{len(test_images)}\")\n",
    "    \n",
    "    try:\n",
    "        response = provider.analyze(img_path, fewshot_prompt)\n",
    "        predicted_score = extract_score_from_response(response)\n",
    "        \n",
    "        results.append({\n",
    "            \"image_path\": img_path,\n",
    "            \"file_name\": row['file_name'],\n",
    "            \"expert_score\": expert_score,\n",
    "            \"predicted_score\": predicted_score,\n",
    "            \"provider\": provider_name,\n",
    "            \"method\": \"few-shot\",\n",
    "            \"raw_response\": response if response else None\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"image_path\": img_path,\n",
    "            \"file_name\": row['file_name'],\n",
    "            \"expert_score\": expert_score,\n",
    "            \"predicted_score\": None,\n",
    "            \"provider\": provider_name,\n",
    "            \"method\": \"few-shot\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "fewshot_results = pd.DataFrame(results)\n",
    "print(f\"\\n\u2705 Completed few-shot scoring: {len(fewshot_results)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "display(fewshot_results.head(10))\n",
    "\n",
    "valid_predictions = fewshot_results[fewshot_results['predicted_score'].notna()]\n",
    "if len(valid_predictions) > 0:\n",
    "    print(f\"\\n\u2705 Valid predictions: {len(valid_predictions)}/{len(fewshot_results)}\")\n",
    "    print(f\"\ud83d\udcca Accuracy: {np.mean(valid_predictions['expert_score'] == valid_predictions['predicted_score']):.2%}\")\n",
    "    print(f\"\\n\ud83d\udcca Predicted score distribution:\")\n",
    "    print(valid_predictions['predicted_score'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize few-shot results\n",
    "valid_df = fewshot_results[fewshot_results['predicted_score'].notna()].copy()\n",
    "\n",
    "if len(valid_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Score distribution comparison\n",
    "    expert_counts = valid_df['expert_score'].value_counts().sort_index()\n",
    "    pred_counts = valid_df['predicted_score'].value_counts().sort_index()\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Expert Score': expert_counts,\n",
    "        'Predicted Score': pred_counts\n",
    "    }).fillna(0)\n",
    "    \n",
    "    comparison.plot(kind='bar', ax=axes[0], color=['skyblue', 'coral'])\n",
    "    axes[0].set_title('Few-Shot: Score Distribution')\n",
    "    axes[0].set_xlabel('Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(valid_df['expert_score'], valid_df['predicted_score'], labels=[1,2,3,4,5])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1], \n",
    "                xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "    axes[1].set_title('Few-Shot: Confusion Matrix')\n",
    "    axes[1].set_xlabel('Predicted Score')\n",
    "    axes[1].set_ylabel('Expert Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{classification_report(valid_df['expert_score'], valid_df['predicted_score'], labels=[1,2,3,4,5])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with zero-shot results if available\n",
    "zeroshot_path = os.path.join(Config.OUTPUTS_DIR, f\"zeroshot_scores_{provider_name}.csv\")\n",
    "\n",
    "if os.path.exists(zeroshot_path):\n",
    "    zeroshot_results = pd.read_csv(zeroshot_path)\n",
    "    zeroshot_valid = zeroshot_results[zeroshot_results['predicted_score'].notna()]\n",
    "    \n",
    "    if len(zeroshot_valid) > 0 and len(valid_df) > 0:\n",
    "        # Compare accuracies\n",
    "        zeroshot_acc = np.mean(zeroshot_valid['expert_score'] == zeroshot_valid['predicted_score'])\n",
    "        fewshot_acc = np.mean(valid_df['expert_score'] == valid_df['predicted_score'])\n",
    "        \n",
    "        print(f\"\ud83d\udcca Comparison:\")\n",
    "        print(f\"  Zero-shot accuracy: {zeroshot_acc:.2%}\")\n",
    "        print(f\"  Few-shot accuracy: {fewshot_acc:.2%}\")\n",
    "        print(f\"  Improvement: {fewshot_acc - zeroshot_acc:+.2%}\")\n",
    "        \n",
    "        # Side-by-side visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        for idx, (method, df_method) in enumerate([(\"Zero-Shot\", zeroshot_valid), (\"Few-Shot\", valid_df)]):\n",
    "            axes[idx].scatter(df_method['expert_score'], df_method['predicted_score'], alpha=0.5)\n",
    "            axes[idx].plot([1, 5], [1, 5], 'r--', label='Perfect')\n",
    "            axes[idx].set_xlabel('Expert Score')\n",
    "            axes[idx].set_ylabel('Predicted Score')\n",
    "            acc = np.mean(df_method['expert_score'] == df_method['predicted_score'])\n",
    "            axes[idx].set_title(f'{method}\\nAccuracy: {acc:.2%}')\n",
    "            axes[idx].legend()\n",
    "            axes[idx].set_xlim(0.5, 5.5)\n",
    "            axes[idx].set_ylim(0.5, 5.5)\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Zero-shot results not found. Run notebook 02 first to compare.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Config.OUTPUTS_DIR\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, f\"fewshot_scores_{provider_name}.csv\")\n",
    "fewshot_results.to_csv(output_path, index=False)\n",
    "print(f\"\u2705 Results saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}