{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis: Subcategory Scores \u2192 Overall Score\n",
    "\n",
    "This notebook analyzes how subcategory scores (from VLM responses) correlate with and can predict the overall property condition score.\n",
    "\n",
    "We'll:\n",
    "1. Load VLM scoring results with subcategory scores\n",
    "2. Analyze correlations between subcategories and overall score\n",
    "3. Train regression models to predict overall score from subcategories\n",
    "4. Visualize feature importance and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Markdown\n",
    "from src.config import Config\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subcategory_scores(response):\n",
    "    \"\"\"Extract subcategory scores from VLM JSON response\"\"\"\n",
    "    if not response:\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Clean response\n",
    "        clean_response = response.strip()\n",
    "        if \"```json\" in clean_response:\n",
    "            clean_response = clean_response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in clean_response:\n",
    "            clean_response = clean_response.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        parsed = json.loads(clean_response)\n",
    "        \n",
    "        # Extract subcategory scores\n",
    "        subscores = {}\n",
    "        if 'subscores' in parsed:\n",
    "            for category, data in parsed['subscores'].items():\n",
    "                if isinstance(data, dict) and 'score' in data:\n",
    "                    subscores[category] = data['score']\n",
    "                elif isinstance(data, (int, float)):\n",
    "                    subscores[category] = data\n",
    "        \n",
    "        return subscores\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "# Load VLM scoring results\n",
    "output_dir = Config.OUTPUTS_DIR\n",
    "results_files = [\n",
    "    os.path.join(output_dir, \"zeroshot_scores_openai.csv\"),\n",
    "    os.path.join(output_dir, \"fewshot_scores_openai.csv\"),\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for file_path in results_files:\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_results.append(df)\n",
    "        print(f\"\u2705 Loaded {len(df)} results from {os.path.basename(file_path)}\")\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\n\ud83d\udcca Total results: {len(results_df)}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No results files found. Run scoring notebooks first.\")\n",
    "    results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subcategory scores from VLM responses\n",
    "if len(results_df) > 0 and 'raw_response' in results_df.columns:\n",
    "    subcategory_data = []\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        if pd.notna(row.get('raw_response')):\n",
    "            subscores = extract_subcategory_scores(row['raw_response'])\n",
    "            \n",
    "            if subscores:\n",
    "                record = {\n",
    "                    'image_path': row.get('image_path'),\n",
    "                    'expert_score': row.get('expert_score'),\n",
    "                    'predicted_score': row.get('predicted_score'),\n",
    "                }\n",
    "                record.update(subscores)\n",
    "                subcategory_data.append(record)\n",
    "    \n",
    "    subcat_df = pd.DataFrame(subcategory_data)\n",
    "    \n",
    "    if len(subcat_df) > 0:\n",
    "        print(f\"\u2705 Extracted subcategory scores from {len(subcat_df)} responses\")\n",
    "        print(f\"\\n\ud83d\udcca Subcategories found:\")\n",
    "        subcat_cols = [c for c in subcat_df.columns if c not in ['image_path', 'expert_score', 'predicted_score']]\n",
    "        print(subcat_cols)\n",
    "        display(subcat_df.head())\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No subcategory scores found in responses\")\n",
    "        subcat_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No results with raw_response column found\")\n",
    "    subcat_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if len(subcat_df) > 0 and 'expert_score' in subcat_df.columns:\n",
    "    subcat_cols = [c for c in subcat_df.columns if c not in ['image_path', 'expert_score', 'predicted_score']]\n",
    "    \n",
    "    if len(subcat_cols) > 0:\n",
    "        # Calculate correlations\n",
    "        correlation_data = []\n",
    "        for col in subcat_cols:\n",
    "            if subcat_df[col].notna().sum() > 0:\n",
    "                corr = subcat_df[col].corr(subcat_df['expert_score'])\n",
    "                correlation_data.append({\n",
    "                    'subcategory': col,\n",
    "                    'correlation': corr,\n",
    "                    'samples': subcat_df[col].notna().sum()\n",
    "                })\n",
    "        \n",
    "        corr_df = pd.DataFrame(correlation_data).sort_values('correlation', ascending=False)\n",
    "        \n",
    "        print(\"\ud83d\udcca Subcategory Correlations with Expert Score:\")\n",
    "        display(corr_df)\n",
    "        \n",
    "        # Visualize correlation heatmap\n",
    "        if len(subcat_cols) > 1:\n",
    "            corr_matrix = subcat_df[subcat_cols + ['expert_score']].corr()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                       square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "            plt.title('Correlation Matrix: Subcategories vs Expert Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No subcategory columns found\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cannot perform correlation analysis - missing data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "if len(subcat_df) > 0 and 'expert_score' in subcat_df.columns:\n",
    "    subcat_cols = [c for c in subcat_df.columns if c not in ['image_path', 'expert_score', 'predicted_score']]\n",
    "    \n",
    "    # Filter to rows with all subcategories and expert score\n",
    "    regression_df = subcat_df[subcat_cols + ['expert_score']].dropna()\n",
    "    \n",
    "    if len(regression_df) > 0:\n",
    "        X = regression_df[subcat_cols]\n",
    "        y = regression_df['expert_score']\n",
    "        \n",
    "        print(f\"\u2705 Prepared {len(regression_df)} samples for regression\")\n",
    "        print(f\"\ud83d\udcca Features: {list(X.columns)}\")\n",
    "        print(f\"\ud83d\udcca Target distribution:\")\n",
    "        print(y.value_counts().sort_index())\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        print(f\"\\n\ud83d\udcca Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  No complete samples for regression\")\n",
    "        X_train, X_test, y_train, y_test = None, None, None, None\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cannot prepare regression data\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression models\n",
    "if X_train is not None:\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'y_test_pred': y_test_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Train R\u00b2: {train_r2:.3f}, Test R\u00b2: {test_r2:.3f}\")\n",
    "        print(f\"  Train MAE: {train_mae:.3f}, Test MAE: {test_mae:.3f}\")\n",
    "    \n",
    "    # Compare models\n",
    "    comparison_df = pd.DataFrame({\n",
    "        name: {\n",
    "            'Test R\u00b2': res['test_r2'],\n",
    "            'Test MAE': res['test_mae'],\n",
    "            'Test MSE': res['test_mse']\n",
    "        }\n",
    "        for name, res in results.items()\n",
    "    }).T\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Model Comparison:\")\n",
    "    display(comparison_df)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cannot train models - missing data\")\n",
    "    results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "if results and X_test is not None:\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 5))\n",
    "    if len(results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, res) in enumerate(results.items()):\n",
    "        axes[idx].scatter(y_test, res['y_test_pred'], alpha=0.6)\n",
    "        axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect')\n",
    "        axes[idx].set_xlabel('Expert Score')\n",
    "        axes[idx].set_ylabel('Predicted Score')\n",
    "        axes[idx].set_title(f'{name}\\nR\u00b2 = {res[\"test_r2\"]:.3f}')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for Random Forest)\n",
    "if 'Random Forest' in results:\n",
    "    rf_model = results['Random Forest']['model']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\ud83d\udcca Feature Importance (Random Forest):\")\n",
    "    display(feature_importance)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Feature Importance: Subcategories \u2192 Overall Score')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}